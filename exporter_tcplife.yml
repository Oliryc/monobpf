# Based on https://github.com/iovisor/bcc/blob/master/tools/tcplife.py
# TODO Remove code used only in tcplife
programs:
  - name: tcp_life
    metrics:
      counters:
        - name: latency
          help: tcp4 connection latency
          table: pid_lat
          labels:
            - name: pid
              size: 4
              decoders:
                - name: uint
            - name: ip46
              size: 4
              decoders:
                - name: uint
                - name: static_map
                  static_map:
                    4: "IPv4"
                    6: "IPv6"
            - name: proc_name
              size: 16
              decoders:
                - name: string
                - name: regexp
                  regexps:
                    - ^httpd$ 
    kprobes:
      tcp_set_state: kprobe__tcp_set_state
    code: |
      //Â /!\ REQUIRES LINUX <= 4.14 (even if it seems to work with liux == 4.15)
      //@begin=cpp@
      #include <uapi/linux/ptrace.h>
      #define KBUILD_MODNAME "foo"
      #include <linux/tcp.h>
      #include <net/sock.h>
      #include <bcc/proto.h>

      BPF_HASH(birth, struct sock *, u64);

      // separate data structs for ipv4 and ipv6
      struct ipv4_data_t {
          u64 ts_us; // 8
          u32 pid; // 4
          u32 saddr; // 4
          u32 daddr; // 4
          u64 ports; // 8
          u64 rx_b; // 8
          u64 tx_b; // 8
          u64 span_us; // 8
          char task[TASK_COMM_LEN];
      };
      BPF_PERF_OUTPUT(ipv4_events);


      struct ipv6_data_t {
          u64 ts_us;
          u32 pid;
          unsigned __int128 saddr;
          unsigned __int128 daddr;
          u64 ports;
          u64 rx_b;
          u64 tx_b;
          u64 span_us;
          char task[TASK_COMM_LEN];
      };
      BPF_PERF_OUTPUT(ipv6_events);

      // Minimal structure for export
      struct export_min_t {
          u32 pid;
          u32 ip46; // Ajouter IPv4/v6, even if we are storing only two values, it takes the size of a u32 with padding
          char task[TASK_COMM_LEN];
      };

      // last latency by pid
      BPF_HASH(pid_lat, struct export_min_t, u64);

      struct id_t {
          u32 pid;
          char task[TASK_COMM_LEN];
      };
      BPF_HASH(whoami, struct sock *, struct id_t);

      // ---------------------------------- after this, depends of kernel version, see tcplife

      int kprobe__tcp_set_state(struct pt_regs *ctx, struct sock *sk, int state)
      {
          u32 pid = bpf_get_current_pid_tgid() >> 32;

          // lport is either used in a filter here, or later
          u16 lport = sk->__sk_common.skc_num;
          // FILTER_LPORT

          // dport is either used in a filter here, or later
          u16 dport = sk->__sk_common.skc_dport;
          dport = ntohs(dport);
          // FILTER_DPORT

          /*
           * This tool includes PID and comm context. It's best effort, and may
           * be wrong in some situations. It currently works like this:
           * - record timestamp on any state < TCP_FIN_WAIT1
           * - cache task context on:
           *       TCP_SYN_SENT: tracing from client
           *       TCP_LAST_ACK: client-closed from server
           * - do output on TCP_CLOSE:
           *       fetch task context if cached, or use current task
           */

          // capture birth time
          if (state < TCP_FIN_WAIT1) {
              /*
               * Matching just ESTABLISHED may be sufficient, provided no code-path
               * sets ESTABLISHED without a tcp_set_state() call. Until we know
               * that for sure, match all early states to increase chances a
               * timestamp is set.
               * Note that this needs to be set before the PID filter later on,
               * since the PID isn't reliable for these early stages, so we must
               * save all timestamps and do the PID filter later when we can.
               */
              u64 ts = bpf_ktime_get_ns();
              birth.update(&sk, &ts);
          }

          // record PID & comm on SYN_SENT
          if (state == TCP_SYN_SENT || state == TCP_LAST_ACK) {
              // now we can PID filter, both here and a little later on for CLOSE
              // FILTER_PID
              struct id_t me = {.pid = pid};
              bpf_get_current_comm(&me.task, sizeof(me.task));
              whoami.update(&sk, &me);
          }

          if (state != TCP_CLOSE)
              return 0;

          // calculate lifespan
          u64 *tsp, delta_us;
          tsp = birth.lookup(&sk);
          if (tsp == 0) {
              whoami.delete(&sk);     // may not exist
              return 0;               // missed create
          }
          delta_us = (bpf_ktime_get_ns() - *tsp) / 1000;
          birth.delete(&sk);

          // fetch possible cached data, and filter
          struct id_t *mep;
          mep = whoami.lookup(&sk);
          if (mep != 0)
              pid = mep->pid;
          // FILTER_PID

          // get throughput stats. see tcp_get_info().
          u64 rx_b = 0, tx_b = 0, sport = 0;
          struct tcp_sock *tp = (struct tcp_sock *)sk;
          rx_b = tp->bytes_received;
          tx_b = tp->bytes_acked;

          u16 family = sk->__sk_common.skc_family;

          if (family == AF_INET) {
              struct ipv4_data_t data4 = {};
              data4.span_us = delta_us;
              data4.rx_b = rx_b;
              data4.tx_b = tx_b;
              data4.ts_us = bpf_ktime_get_ns() / 1000;
              data4.saddr = sk->__sk_common.skc_rcv_saddr;
              data4.daddr = sk->__sk_common.skc_daddr;
              // a workaround until data4 compiles with separate lport/dport
              data4.pid = pid;
              data4.ports = dport + ((0ULL + lport) << 32);
              if (mep == 0) {
                  bpf_get_current_comm(&data4.task, sizeof(data4.task));
              } else {
                  bpf_probe_read(&data4.task, sizeof(data4.task), (void *)mep->task);
              }
              ipv4_events.perf_submit(ctx, &data4, sizeof(data4));

              // Record latency
              struct export_min_t export_data = {.pid = pid, .ip46 = 4};
              __builtin_memcpy(&export_data.task, data4.task, sizeof(export_data.task));
              u64 latency_val = delta_us;
              u64 *last_latency_val_p = pid_lat.lookup(&export_data);
              if (last_latency_val_p != 0) {
                pid_lat.update(&export_data, &latency_val);
              } else {
                pid_lat.insert(&export_data, &latency_val);
              }

          } else /* 6 */ {
              struct ipv6_data_t data6 = {};
              data6.span_us = delta_us;
              data6.rx_b = rx_b;
              data6.tx_b = tx_b;
              data6.ts_us = bpf_ktime_get_ns() / 1000;
              bpf_probe_read(&data6.saddr, sizeof(data6.saddr),
                  sk->__sk_common.skc_v6_rcv_saddr.in6_u.u6_addr32);
              bpf_probe_read(&data6.daddr, sizeof(data6.daddr),
                  sk->__sk_common.skc_v6_daddr.in6_u.u6_addr32);
              // a workaround until data6 compiles with separate lport/dport
              data6.ports = dport + ((0ULL + lport) << 32);
              data6.pid = pid;
              if (mep == 0) {
                  bpf_get_current_comm(&data6.task, sizeof(data6.task));
              } else {
                  bpf_probe_read(&data6.task, sizeof(data6.task), (void *)mep->task);
              }
              ipv6_events.perf_submit(ctx, &data6, sizeof(data6));

              // Record latency
              struct export_min_t export_data = {.pid = pid, .ip46 = 6};
              __builtin_memcpy(&export_data.task, data6.task, sizeof(export_data.task));
              u64 latency_val = delta_us;
              u64 *last_latency_val_p = pid_lat.lookup(&export_data);
              if (last_latency_val_p != 0) {
                pid_lat.update(&export_data, &latency_val);
              } else {
                pid_lat.insert(&export_data, &latency_val);
              }

          }

          if (mep != 0)
              whoami.delete(&sk);

          return 0;
      }

      //@end=ccp@
